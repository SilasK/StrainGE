#!/usr/bin/env python
"""StrainGR"""

import argparse
import math
import os
import sys
import numpy as np
from scipy.stats import norm
import pysam
import kmertools



parser = argparse.ArgumentParser(description="""
Strain Genome Recovery (StrainGR) Tool
""")
parser.add_argument("--table", "-t", action="store_true",
                    help="generate tabular output in <bambasename>.stats.tsv")
parser.add_argument("--compare", "-c", action='store_true',
                    help="compare against first listed strain bam")
parser.add_argument("--details", "-d", action='count',
                    help="log specific differences between reference and strain(s) in <bambasename>.details.tsv; if used twice, include ref matches")
parser.add_argument("--verbose", "-v",
                    help="increase output verbosity", action="store_true")
parser.add_argument("--minqual", type=int, default=5,
                    help='minimum base Quality score to consider')
parser.add_argument("--minmq", type=int, default=5,
                    help='minimum read Mapping quality score to consider')
parser.add_argument("--minconfirm", type=int, default=50,
                    help="minimum Pileup quality sum to confirm reference or SNP (with minfrac)")
parser.add_argument("--mingap", type=int, default=2000,
                    help="minimum contiguous uncovered (Gap) region to flag")
parser.add_argument("--minfrac", type=float, default=0.1,
                    help="minimum fraction of evidence to confirm presence (with minconfirm)")
parser.add_argument("--gaps", "-g",
                    help="print gaps (large uncoverged regions)", action="store_true")
parser.add_argument("--log", "-l",
                    help="log file")
parser.add_argument("reference",
                    help="reference FASTA file")
parser.add_argument("bam", nargs='+',
                    help="bam file of reads aligned to reference (sorted & indexed)")
args = parser.parse_args()

logfile = open(args.log, 'w') if args.log else sys.stdout


class Reference:
    """
    Holds reference sequence information...scaffolds in Biopython SeqRecord objects.
    """
    def __init__(self, fasta):
        self.scaffolds = [scaffold for scaffold in kmertools.openSeqFile(fasta)]
        self.lengths = [len(s) for s in self.scaffolds]
        self.length = sum(self.lengths)
        self.byname = {scaffold.name: scaffold for scaffold in self.scaffolds}

        print >> logfile, "Reference has", len(self.scaffolds), "scaffolds containing", self.length, "bases"


    def scaffoldCoord(self, coord):
        """
        Turn a zero-based genome-wide coordinate into a scaffold & coordinate within scaffold (1-based)
        :param coord: zero-based genome-wide coordinate
        :return: (scaffold, scaffoldCoord)
        """
        offset = 0
        for scaffold, length in zip(self.scaffolds, self.lengths):
            if coord < offset + length:
                return (scaffold.name, coord + 1 - offset)
            offset += length

    def scaffoldToGenomeCoord(self, scaffoldName, coord):
        """
        Turn a 1-based scaffold coordinate into a 0-based genome-wide coordinate
        :param scaffoldName: scaffold name
        :param coord: 1-based scaffold coordinate
        :return: genomeCoord
        """
        offset = 0
        for scaffold, length in zip(self.scaffolds, self.lengths):
            if scaffold.name == scaffoldName:
                return offset + coord - 1
            offset += length

    def getSeq(self, name, coord, length=1):
        return self.byname[name].seq[coord-1:coord+length-1]


class Sample:
    """
    Collects and holds information about alignments of sample reads (from a supplied BAM file) against a reference.
    """
    def __init__(self, reference, bamfile):
        self.reference = reference
        self.length = reference.length
        self.bamfile = bamfile

        # These are filled out after allele calling. Most are numpy arrays with one element for each genomic position.
        self.scaffolds = None
        self.refmask = None
        self.coverage = None
        self.lowmq = None
        self.highCoverage = None
        self.strong = None
        self.weak = None
        self.gaps = []
        self.summaryStats = None

        self.process(bamfile)

    def process(self, bam):
        """
        Does the heavy lifting of scanning the BAM file. The basic flow is:
        1) process the alignments on a scaffold-by-scaffold basis, generating ScaffoldsPileups objects
        2) generate some summary statistics and heuristic thresholds (e.g., read depth cutoff)
        3) scan through the ScaffoldPileups and make calls
        4) compute summary call statistics for each scaffold as well as the total for the genome

        :param bam: BAM file containing reads for a sample aligned to the reference genome.
        """
        print >>logfile, "Processing", bam

        # Create ScaffoldPileups objects for each scaffold, and scan the bam to fill them out.
        self.scaffolds = [ScaffoldPileups(scaffold.name, scaffold.seq.upper()) for scaffold in reference.scaffolds]
        scaffoldsByName = {s.name: s for s in self.scaffolds}
        with pysam.AlignmentFile(bamfile, "rb") as bam:
            for s in self.scaffolds:
                s.makePileups(bam, scaffoldsByName)

        # Pull together essential stats for the whole genome so we can set thresholds
        self.refmask = np.concatenate([s.refmask for s in self.scaffolds])
        self.coverage = np.concatenate([s.coverage for s in self.scaffolds])
        meanCoverage = float(self.coverage.sum()) / float(self.length)
        medianCoverage = float(np.median(self.coverage))


        print >>logfile, "Total length=%d coverage=%.2f median=%.1f" % (self.length, meanCoverage, medianCoverage),

        nonzero = self.coverage[self.coverage > 0]
        if nonzero.size > 0:
            nonzeroCoverage = float(nonzero.sum()) / float(nonzero.size)
        else:
            nonzeroCoverage = 0
        print >>logfile, "nonzero=%d nzcoverage=%.2f" % (nonzero.size, nonzeroCoverage)
        logfile.flush()

        # Scan through ScaffoldPileups, make calls using the thresholds set above, and compute summary stats
        for s in self.scaffolds:
            s.callAlleles()
            self.gaps += s.findGaps()
            s.stats()

        # Now gather the essentials for the whole genome so we can compute summary stats.
        self.lowmq = np.concatenate([s.lowmq for s in self.scaffolds])
        self.highCoverage = np.concatenate([s.highCoverage for s in self.scaffolds])
        self.strong = np.concatenate([s.strong for s in self.scaffolds])
        self.weak = np.concatenate([s.weak for s in self.scaffolds])

        self.summaryStats = CallStats(self.length, self.refmask, self.coverage, self.strong, self.lowmq, self.highCoverage, self.gaps)
        self.summaryStats.log("TOTAL")
        if args.gaps:
            for g in self.gaps:
                print >>logfile, "Gap:", g, \
                    self.reference.scaffoldToGenomeCoord(g.id, g.start), \
                    self.reference.scaffoldToGenomeCoord(g.id, g.stop)

        # If tabular output has been requested, create a tab-separated values output file with entries
        # for each scaffold and total.
        if args.table:
            fileName = self.strainName() + ".stats.tsv"
            print >>logfile, "Writing stats to", fileName
            logfile.flush()
            with open(os.path.join(os.path.dirname(self.bamfile), fileName), 'w') as outfile:
                self.summaryStats.tabular(outfile, "TOTAL", True)
                for s in self.scaffolds:
                    s.summaryStats.tabular(outfile, s.name)
                self.summaryStats.tabular(outfile, "TOTAL")

        if args.details:
            fileName = self.strainName() + ".details.tsv"
            print >>logfile, "Writing allele differences to", fileName
            logfile.flush()
            with open(os.path.join(os.path.dirname(self.bamfile), fileName), 'w') as outfile:
                self.details(outfile)


    def strainName(self):
        """
        Derive "strain name" from basename of bam file...used to derive other output file names
        :return: basename suitable for appending output file extensions
        """
        basename = os.path.basename(self.bamfile)
        if basename.lower().endswith(".bam"):
            basename = basename[:-4]
        return basename

    def details(self, outfile):
        """
        Write allele differences from reference to output file. Example:
        NZ_CP012633.1	1876026	A	C
        NZ_CP012633.1	2247779	G	T/G
        NZ_CP012633.1	2730361	C	C,G

        :param outfile: output file
        """
        print >>outfile, "\t".join(["scaffold", "coord", "ref", "alleles"])
        for i in xrange(self.length):
            mask = self.strong[i]
            # if double-details, then include ref matches!
            if mask and (mask != self.refmask[i] or args.details > 1):
                scaffold, coord = self.reference.scaffoldCoord(i)
                ref = self.maskString(self.refmask[i])
                alleles = self.strongWeakString(self.strong[i], self.weak[i])
                print >>outfile, "\t".join([scaffold, str(coord), ref, alleles])

    def compareDetails(self, other, outfile):
        """
        Write allele differences between this and another strain to output file. Example:
        NZ_CP012633.1	98692	A	C	A
        NZ_CP012633.1	99146	G	G	G,T
        NZ_CP012633.1	122725	T	A/C,T	T

        :param other: other strain
        :param outfile: output file
        """
        print >>outfile, "\t".join(["scaffold", "coord", "ref", self.strainName(), other.strainName()])
        for i in xrange(self.length):
            mask = self.strong[i]
            otherMask = other.strong[i]
            if mask and otherMask and (mask != otherMask or args.details > 1):
                scaffold, coord = self.reference.scaffoldCoord(i)
                ref = self.maskString(self.refmask[i])
                alleles = self.strongWeakString(self.strong[i], self.weak[i])
                otherAlleles = self.strongWeakString(other.strong[i], other.weak[i])
                print >>outfile, "\t".join([scaffold, str(coord), ref, alleles, otherAlleles])

    def strongWeakString(self, strong, weak):
        """
        Return a string representing strong/weak alleles. Multiple alleles will be separted by commas. If there
        are weak alleles which aren't strong, they will be separtaed from the strong alleles by a '/'.
        Example: A,C/T (A & C are strong, T is weak)
        :param strong: string mask
        :param weak: weak mask
        :return: strong/weak allele string
        """
        s = self.maskString(strong)
        w = weak & ~strong
        if w:
            s += '/' + self.maskString(w)
        return s

    def maskString(self, mask):
        """
        Turn an allele bitmask into a string of alleles
        :param mask: allele bitmask
        :return: string representation of alleles
        """
        if mask:
            return ','.join([Alleles.ALLELES[a] for a in xrange(Alleles.N_ALLELES) if mask & Alleles.ALLELE_MASKS[a]])
        else:
            return ''

class Alleles:
    # bases and their indicies (plus I for insertion, D for deletion)
    ALLELES = "ACGTID"
    A = 0
    C = 1
    G = 2
    T = 3
    I = 4
    D = 5
    N_ALLELES = len(ALLELES)
    # we note presence of alleles by setting a bit corresponding to the allele index above
    ALLELE_MASKS = 1 << np.arange(N_ALLELES, dtype=np.int32)

class ScaffoldPileups:
    """
    Object to compute an hold all the pileups for each location in an input genome element (scaffold).
    Detail information is contained in Pileup objects for each locus; summary information is in numpy arrays (coverage,
    strong, weak, ...) with one element per locus.
    """


    def __init__(self, name, sequence):
        self.name = name
        self.sequence = sequence
        self.length = len(sequence)

        # large memory requirement done this way (object per locus). could be refactored by having multidimensional
        # array with allele evidence for the whole scaffold.
        #self.pileups = np.array([Pileup(base) for base in sequence], dtype=np.dtype(object))
        self.refmask = np.zeros(self.length, dtype=np.uint8)
        for i, base in enumerate(sequence):
            refindex = Alleles.ALLELES.find(base)
            # allele bitmask corresponding to reference base
            self.refmask[i] = (1 << refindex) if refindex >= 0 else 0

        # pileup stats
        # second dimension is count, qualsum
        self.alleles = np.zeros((self.length, 2, Alleles.N_ALLELES), dtype=np.int32)
        # good alignments at this position
        self.coverage = np.zeros(self.length, dtype=np.uint32)
        # alignments covering this position but rejected for some reason (bad pairing, poor quality, etc)
        self.bad = np.zeros(self.length, dtype=np.uint32)
        # otherwise good alignments with low mapping quality (probably because this is a repeat)
        self.lowmqCount = np.zeros(self.length, dtype=np.uint32)
        self.lowmq = np.zeros(self.length, dtype=np.uint32)
        # allele calls: strong and weak are both byte arrays with one bit set for each present allele (A,C,G,T + ins + del)
        self.strong = np.zeros(self.length, dtype=np.uint8)
        self.weak = np.zeros(self.length, dtype=np.uint8)

        # allele call arrays computed here
        self.highCoverage = None
        self.summaryStats = None
        # list of Region objects for uncovered regions
        self.gaps = []

        # initialize refindex

    def makePileups(self, bam, otherScaffoldPileups):
        """
        Loop over loci and add the info from each covering read. Also sets count of "good" coverage and refmask, which
        is the bitmask corresponding to the reference allele for each comparison later.

        :param bam: open bam file (pysam object).
        """
        print >>logfile, "Generating pileups for", self.name, "length=%d" % self.length,
        logfile.flush()

        for column in bam.pileup(self.name):
            refpos = column.reference_pos
            otherLowMq = self.add_reads(refpos, column.pileups)
            for otherName, otherCoord in otherLowMq:
                otherPileups = otherScaffoldPileups[otherName]
                if otherCoord <= otherPileups.lowmq.size:
                    otherScaffoldPileups[otherName].lowmq[otherCoord-1] += 1

        for i in xrange(self.length):
            self.coverage[i] = self.depth(i)

        # When we look for blocks without coverage (possible large deletions from recombination events), we attempt
        # to scale the minimum size to be significant, because at very low coverage, we expect large uncovered regions.
        meanCoverage = float(self.coverage.sum()) / float(self.length)
        self.mingap = self.gapCutoff(meanCoverage)

        # Based on coverage, set a high limit for plausible coverage of a given position based on Poisson probability;
        # because we are dealing with metagenomic samples, too improbabily high coverage is likely a result of reads
        # from other organisms aligning to conserved regions, so we can't make a confident call on our target organism.
        # Here, we use the median coverage rather than the mean since our coverage might be dominated by conserved regions.
        medianCoverage = np.median(self.coverage)
        self.highCutoff = self.poissonCutoff(max(medianCoverage, 0.5))
        self.highCoverage = self.coverage > self.highCutoff

        print >>logfile, "coverage=%.2f median=%d cutoff=%d mingap=%d" % (meanCoverage, medianCoverage, self.highCutoff, self.mingap)
        logfile.flush()

    def callAlleles(self):
        """
        Loop over loci compiling evidence for alleles present. Ignore pileups deeper than highCutoff, because those
        probably contain alignments from other organisms (e.g., this might be a conserved region).

        "strong" means there is suffiently strong evidnece to "call" the allele as present
        "weak" means any evidence at all supporting the allele, even if not strong enought to be strong

        :param highCutoff: max pileup depth
        """
        print >>logfile, "Calling alleles in", self.name,
        logfile.flush()

        for i in xrange(self.length):
            if self.coverage[i] and not self.highCoverage[i]:
                self.callAllelesLoc(i)


    def findGaps(self):
        """
        Find coverage gaps, indicating possible deleted portions in our strain vs the reference (e.g., possible
        recombination events). Gaps are defined here as insufficient evidence to call anything, taking into account
        regions of poor mapping quality won't be called as a matter of course (but may still be "covered").

        :param mingap: don't report gaps unless they consist of this many consecutive loci which can't be called
        :return: list of Region objects corresponding to the coverage gaps, also set as self.gaps
        """
        lastCovered = -1
        for i in xrange(self.length):
            # heuristic: more low mapping quality than "good" coverage
            self.lowmq[i] = self.lowmqCount[i] > 1 and self.lowmqCount[i] > self.depth(i)
            # we break at either a call or a location which has sufficient coverage at poor mapping quality (repeats)
            if self.strong[i] or self.lowmq[i]:
                if lastCovered >= 0 and i - lastCovered > self.mingap:
                    self.gaps.append(Region(self.name, lastCovered+1, i-1))
                lastCovered = i
        if self.length - lastCovered >= self.mingap:
            self.gaps.append(Region(self.name, lastCovered + 1, self.length - 1))
        return self.gaps


    def gapCutoff(self, coverage):
        """
        Attempt to scale the minimum significant uncoveraged region by coverage (for low coverage)
        :param coverage: expected coverage
        :return: scaled minimum gap (using args.mingap as baseline)
        """
        lw = landerWaterman(coverage)
        return args.mingap / lw if lw > 0 else args.mingap

    def poissonCutoff(self, mean, cutoff = 0.9999999):
        """
        Calculate the Poisson CDF and find where it reaches the cutoff. For higher coverages, use linear instead.
        Default cutoff is one part in 10M, so not likely to occur in a typical bacterial genome.
        """
        if mean < 50:
            # for lower coverage, use poisson
            i = 0
            ifact = 1.0
            expmean = math.exp(-mean)
            cdf = 0.0
            while cdf < cutoff:
                cdf += expmean * math.pow(mean, float(i)) / ifact
                i += 1
                ifact *= i
            return i - 1
        else:
            return int(math.ceil(mean * 1.5 + 15.0))


    def add_reads(self, loc, pileup_reads):
        """Add a bunch of pysam.PileupRead objects to this pileup"""
        otherLowMq = []
        for read in pileup_reads:
            others = self.add_read(loc, read)
            if others:
                otherLowMq += others
        return otherLowMq

    def add_read(self, loc, read):
        """Add a pysam.PileupRead object to this pileup"""
        alignment = read.alignment

        # if this is a paired read, make sure the pairs are properly aligned
        if alignment.is_paired and not alignment.is_proper_pair:
            self.bad[loc] += 1
            return

        # restrict ourselves to full-length alignments (not clipped)
        if alignment.query_alignment_length != alignment.query_length:
            # alignment is clipped
            self.bad[loc] += 1
            return

        # check that inferred insert size is at least read length
        if alignment.is_paired:
            tlen = alignment.template_length
            if abs(tlen) < alignment.query_length:
                self.bad[loc] += 1
                return

        # get base quality (note this is next base if deletion, but we won't use that)
        pos = read.query_position_or_next
        qual = alignment.query_qualities[pos]
        if qual < args.minqual:
            self.bad[loc] += 1
            return

        # check for decent mapping quality
        mq = alignment.mapping_quality

        # we keep track of otherwise good reads with low mapping quality; that probably means this is a repeat
        if mq < args.minmq:
            self.lowmqCount[loc] += 1
            # experimental: process other mappings
            return self.get_xa(alignment, loc)

        if read.indel:
            if read.is_del:
                # deletion
                baseIndex = Alleles.D
                # if the base isn't there, base qual doesn't make sense, so just use mq
                #qual = mq
            else:
                # then it must be an insertion
                baseIndex = Alleles.I
        else:
            # base call must be real base (e.g., not N)
            base = alignment.query_sequence[pos]
            baseIndex = Alleles.ALLELES.find(base)
            if baseIndex < 0:
                self.bad[loc] += 1
                return

        # We're good! Update the pileup stats...
        #q = min(qual, mq)
        self.alleles[loc, 0, baseIndex] += 1
        self.alleles[loc, 1, baseIndex] += qual

    def callAllelesLoc(self, loc):
        """
        Calls alleles present at this locus.
        "strong" contains the bits of alleles with sufficient evidence for a confident call
        "weak" contains those with any evidence at all
        "confirmed" is set if there is strong evidence for reference base
        """
        minconfirm = args.minconfirm
        minfrac = args.minfrac

        quals = self.alleles[loc, 1]
        qualsum = float(quals.sum())
        # qualfraction is array containing fraction of evidence for each allele
        qualfraction = quals / qualsum

        # Any alleles which have any evidence at all
        evidence = quals > 0
        # boolean * mask will give array of 0 where boolean is false, else allele, then sum() sets them all!
        self.weak[loc] = (evidence * Alleles.ALLELE_MASKS).sum()

        # Alleles with sufficient evidence to confirm (minimum qual sum and minimum qual fraction)
        confirmed = np.logical_and(quals > minconfirm, qualfraction > minfrac)
        self.strong[loc] = (confirmed * Alleles.ALLELE_MASKS).sum()

        if args.verbose and self.strong[loc] and self.refmask[loc] != self.strong[loc]:
            print self.refmask[loc], self.alleles[loc, 0], self.alleles[loc, 1], \
                qualfraction, self.weak[loc], self.strong[loc]


    def get_xa(self, read, loc):
        alns = []
        if read.has_tag("XA"):
            xa = read.get_tag("XA")
            nm = int(read.get_tag("NM"))
            offset = read.reference_end - 1 - loc if read.is_reverse else loc - read.reference_start
            for aln in xa.split(';'):
                if aln:
                    scaffold, pos, cigar, anm = aln.split(',')
                    anm = int(anm)
                    if anm <= nm:
                        pos = int(pos)
                        coord = -pos + read.query_length - 1 - offset if pos < 0 else pos + offset
                        alns.append((scaffold, coord))
        return alns

    def depth(self, loc):
        """
        :return: Count of all good reads (int)
        """
        return self.alleles[loc, 0].sum()

    def qual_total(self, loc):
        """
        :return: Sum of all quality evidence (int)
        """
        return self.alleles[loc, 1].sum()

    def total_depth(self, loc):
        """
        :return: Count of all reads, including those rejected for some reason (int)
        """
        return self.depth(loc) + self.bad[loc]

    def covered(self, loc):
        """
        :return: Does this pileup have enough data to consider this locus covered? (boolean)
        """
        return self.qual_total(loc) >= args.minconfirm

    def ref_qual(self, loc):
        """
        :return: sum of quality evidence for reference base (int)
        """
        return self.alleles[loc, 1, self.refindex]

    def ref_fraction(self, loc):
        """
        :return: Fraction of evidence which supports reference base (float)
        """
        return float(self.ref_qual(loc)) / float(self.qual_total(loc))

    def confirmed(self, loc):
        """
        :return: Does this pileup confirm the reference? (boolean)
        """
        rq = self.ref_qual(loc)
        return rq >= args.minconfirm and (rq == self.qual_total(loc) or self.ref_fraction(loc) > args.consensus)

    def stats(self):
        """
        Populate and report call statistics for this scaffold.
        """
        self.summaryStats = CallStats(self.length, self.refmask, self.coverage, self.strong, self.lowmq, self.highCoverage, self.gaps)
        self.summaryStats.log()


class CallStats:
    """
    Class to compute and log lots of stats (for scaffold or whole genome) given a few summary pileup arrays.
    """

    # these are al the fields and formats in the order we want to show them
    FIELDS = (("length", "%d"), ("coverage", "%.2f"), ("median", "%d"), ("covered", "%d"), ("coveredPct", "%.2f"), ("evenness", "%.2f"),
              ("confirmed", "%d"), ("confirmedPct", "%.2f"), ("snps", "%d"), ("snpPct", "%.2f"), ("multi", "%d"),
              ("multiPct", "%.2f"), ("lowmq", "%d"), ("lowmqPct", "%.2f"), ("high", "%d"), ("highPct", "%.2f"),
              ("gapCount", "%d"), ("gapLength", "%d"))

    def __init__(self, length, refmask, coverage, strong, lowmq, high, gaps):
        # locaitons where we have strong evidence of reference base
        confirmed = (strong & refmask)
        # locations where we have strong evidence of something other than reference (not mutually exclusive!)
        snps = (strong & ~refmask)
        # locations where we have strong evidnece for multiple bases (x&(x-1) turns off only highest bit in x :))
        multi = (strong & (strong - 1)) > 0
        self.length = length
        # covered if we have any strong call
        self.covered = np.count_nonzero(strong)
        self.coveredPct = pct(self.covered, length)
        # exclude high coverage areas when computing mean coverage
        self.coverage = float(coverage[high==0].sum()) / float(length)
        self.median = int(round(np.median(coverage)))
        # evenness is measure of expected "covered" vs coverage (low if coverage concentrated in small portion)
        lw = landerWaterman(self.coverage)
        self.evenness = self.coveredPct / (100.0 * lw * lw) if lw > 0 else 0
        # confirmed count
        self.confirmed = np.count_nonzero(confirmed)
        self.confirmedPct = pct(self.confirmed, self.covered)
        # snp count
        self.snps = np.count_nonzero(snps)
        self.snpPct = pct(self.snps, self.covered)
        # multi count
        self.multi = np.count_nonzero(multi)
        self.multiPct = pct(self.multi, self.snps)
        # low mapping quality
        self.lowmq = np.count_nonzero(lowmq)
        self.lowmqPct = pct(self.lowmq, length)
        # count of locations not called because coverage too high
        self.high = np.count_nonzero(high)
        self.highPct = pct(self.high, length)
        # count of gaps (uncovered reginos) and their cumulative length
        self.gapCount = len(gaps)
        self.gapLength = sum([g.length for g in gaps])

        # values correspond to order of FIELDS above
        self.values = (self.length, self.coverage, self.median, self.covered, self.coveredPct, self.evenness, self.confirmed,
                       self.confirmedPct, self.snps, self.snpPct, self.multi, self.multiPct, self.lowmq, self.lowmqPct,
                       self.high, self.highPct, self.gapCount, self.gapLength)

    def log(self, name=None):
        if name:
            first = [name]
        else:
            first = []
        s = ' '.join(first + [(field[0] + '=' + field[1]) % value for field, value in zip(CallStats.FIELDS, self.values)])
        print >>logfile, s
        logfile.flush()

    def tabular(self, out, name, header=False):
        # write out in tabular output file format (just spit out header if flag set)
        if header:
            print >>out, "\t".join(["name"] + [field for field, fmt in CallStats.FIELDS])
        else:
            print >>out, "\t".join([name] + [ff[1] % value for ff, value in zip(CallStats.FIELDS, self.values)])


class CompareStats:
    """
    Class to compare stats from two samples
    """

    # list of fields and formats, a la CallStats
    FIELDS = [("common", "%d"), ("commonPct", "%.2f"), ("disagree", "%d"), ("disagreePct", "%.2f"), ("variants", "%d"), \
              ("variantPct", "%.2f"), ("commonVariant", "%d"), ("commonVariantPct", "%.2f"), ("variantAgree", "%d"),
              ("variantAgreePct", "%.2f"), ("AnotB", "%d"), ("AnotBpct", "%.2f"), \
              ("AnotBweak", "%d"), ("AnotBweakPct", "%.2f"), ("BnotA", "%d"), ("BnotApct", "%.2f"), \
              ("BnotAweak", "%d"), ("BnotAweakPct", "%.2f"), ("Agaps", "%d"), ("AsharedGaps", "%d"), ("AgapPct", "%.2f"), \
              ("Bgaps", "%d"), ("BsharedGaps", "%d"), ("BgapPct", "%.2f")]

    def __init__(self, strainA, strainB):
        # takes CallStats from strainA, strainB
        self.strainA = strainA
        self.strainB = strainB

        # gather values, a la CallStats
        self.values = self.compare(strainA, strainB)
        self.values += self.compareGaps()

    def compare(self, strainA, strainB):
        """
        :param strainA: CallStats object for strainA
        :param strainB: CallStats object for strainB
        :return: list of computed values
        """
        print >>logfile, "Comparing", strainA.bamfile, "versus",
        logfile.flush()

        # locations where both have a call in common
        common, commonCount, commonPct = self.compareThing(np.ones_like(strainA.refmask), np.logical_and(strainA.strong, strainB.strong), "common")

        # common locations where A & B disagree
        disagree, disagreeCount, disagreePct = self.compareThing(common, strainA.strong != strainB.strong, "disagree")

        # common locations where either has a variant from reference
        variants, variantCount, variantPct = self.compareThing(common, ((strainA.strong | strainB.strong) & ~strainA.refmask) > 0, "variant")

        # variant locations where both have a shared variant
        commonVariants, commonVariantCount, commonVariantPct = self.compareThing(variants, (strainA.strong & strainB.strong) > 0, "commonvariant")

        # variant locations where both agree
        variantAgree, variantAgreeCount, variantAgreePct = self.compareThing(variants, strainA.strong == strainB.strong, "variantAgree")

        # variant in A but not B
        AnotB, AnotBcount, AnotBpct = self.compareThing(variants, (strainA.strong & ~strainB.strong & ~strainA.refmask) > 0, "AnotB")

        # variant in A but not B weakly
        AnotBweak, AnotBweakCount, AnotBweakPct = self.compareThing(variants, (strainA.strong & ~strainB.weak & ~strainA.refmask) > 0, "AnotBweak")

        # variant in B not A
        BnotA, BnotAcount, BnotApct = self.compareThing(variants, (strainB.strong & ~strainA.strong & ~strainA.refmask) > 0, "BnotA")

        # variant in B not A weakly
        BnotAweak, BnotAweakCount, BnotAweakPct = self.compareThing(variants, (strainB.strong & ~strainA.weak & ~strainA.refmask) > 0, "BnotAweak")

        return [commonCount, commonPct, disagreeCount, disagreePct, variantCount, variantPct,
                commonVariantCount, commonVariantPct,
                variantAgreeCount, variantAgreePct, AnotBcount, AnotBpct, AnotBweakCount, AnotBweakPct,
                BnotAcount, BnotApct, BnotAweakCount, BnotAweakPct]

    def compareThing(self, common, thing, label):
        """
        Computes occurance of a condition within a set, and returns those stats.

        :param common: flag for locations to consider
        :param thing: condition we're looking for in common
        :param label: name for this comparison
        :return: array where command and condition are true, count of that, and percentage with respect to common
        """
        commonCount = np.count_nonzero(common)
        commonThings = np.logical_and(common, thing)
        commonThingsCount = np.count_nonzero(commonThings)
        percent = pct(commonThingsCount, commonCount)
        return commonThings, commonThingsCount, percent

    def compareGaps(self):
        """
        More stats, this time about uncovered regions in common.
        :return: list of gap stats
        """
        Agaps = self.strainA.gaps
        Bgaps = self.strainB.gaps
        Alength = sum([g.length for g in Agaps])
        Blength = sum([g.length for g in Bgaps])
        Ashared = filter(lambda ag: any([ag.overlaps(bg) for bg in Bgaps]), Agaps)
        AsharedLength = sum([g.length for g in Ashared])
        Bshared = filter(lambda bg: any([bg.overlaps(ag) for ag in Agaps]), Bgaps)
        BsharedLength = sum([g.length for g in Bshared])
        return [Alength, AsharedLength, pct(AsharedLength, Alength), Blength, BsharedLength, pct(BsharedLength, Blength)]

    def log(self, name):
        """
        Log stats to console.
        :param name: string to put in front of the stats
        """
        s = ' '.join([name] + [(field[0] + '=' + field[1]) % value for field, value in zip(CompareStats.FIELDS, self.values)])
        print >>logfile, s
        logfile.flush()

    def tabular(self, out, name, header=False):
        """
        Log stats to tabular (tsv) output.
        :param out: file object to write the record to
        :param name: string for first colummn
        :param header: print header instead of stats
        """
        if header:
            print >>out, "\t".join(["name"] + [field for field, fmt in CompareStats.FIELDS])
        else:
            print >>out, "\t".join([name] + [ff[1] % value for ff, value in zip(CompareStats.FIELDS, self.values)])


class Region:
    """
    Class to define a consecutive genomic region, defined as a name (e.g., scaffold name), start & stop positions.
    """

    def __init__(self, id, start, stop):
        self.id = id
        self.start = start
        self.stop = stop
        self.length = stop - start + 1
        assert self.length > 0, "Region length must be positive"

    def overlaps(self, other):
        """
        :param other: another Region
        :return: True if they overlap
        """
        return self.id == other.id and other.start <= self.stop and other.stop >= self.start

    def distance(self, other):
        """
        :param other: another Region
        :return: how far apart they are (must be from same element)
        """
        assert self.id == other.id
        return min(abs(self.end - other.start), abs(other.end - self.start)) if not self.overlap() else 0

    def combine(self, other):
        """
        Merge regions
        :param other: another Region
        :return: merged region
        """
        assert self.id == other.id
        return Region(self.id, min(self.start, other.start), max(self.end, other.end))

    def __repr__(self):
        return "<%s:%d-%d %d>" % (self.id, self.start, self.stop, self.stop+1-self.start)

def landerWaterman(coverage):
    """
    Use Lander-Waterman relation to compute expect fraction of genome covered given mean coverage.
    :param coverage: mean coverage
    :return: expected fraction of genome covered
    """
    return 1.0 - math.exp(-coverage)

def pct(numerator, denominator, precision=None):
    """Makes into a percent, avoiding division by zero"""
    if numerator > 0 and denominator > 0:
        value = (100.0 * numerator) / denominator
    else:
        value = 0.0
    if precision is not None:
        value = round(value, precision)
    return value

##################################
### Main
##################################

print >>logfile, "Loading reference genome", args.reference
logfile.flush()
reference = Reference(args.reference)
baseline = None
compareFile = None

for bamfile in args.bam:
    sample = Sample(reference, bamfile)
    sampleName = sample.strainName()
    if args.compare:
        # if we're comparing, use first sample bam as baseli
        if not baseline:
            baseline = sample
        else:
            compare = CompareStats(baseline, sample)
            compare.log(sampleName)
            if not compareFile:
                # create comparison output file and print header
                compareFile = open(os.path.join(os.path.dirname(bamfile), baseline.strainName()) + ".compare.tsv", 'w')
                compare.tabular(compareFile, "sample", True)
            # write tabular output
            compare.tabular(compareFile, sampleName)
            compareFile.flush()
            if args.details:
                with open(os.path.join(os.path.dirname(bamfile), baseline.strainName() + '-' + sample.strainName()) + ".compare.details.tsv", 'w') as compareDetails:
                    baseline.compareDetails(sample, compareDetails)
