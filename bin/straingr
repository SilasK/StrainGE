#!/usr/bin/env python
"""StrainGR"""

import argparse
import math
import os
import sys
from datetime import date
import numpy as np
import pysam
import kmertools



parser = argparse.ArgumentParser(description="""
Strain Genome Recovery (StrainGR) Tool
""")
parser.add_argument("--table", "-t", action="store_true",
                    help="generate tabular output in <bambasename>.tsv")
parser.add_argument("--compare", "-c", action='store_true',
                    help="compare against first listed strain bam")
parser.add_argument("--delta", "-d", action='store_true',
                    help="log specific differences between strains")
parser.add_argument("--verbose", "-v",
                    help="increase output verbosity", action="store_true")
parser.add_argument("--minqual", type=int, default=5,
                    help='minimum base Quality score to consider')
parser.add_argument("--minmq", type=int, default=5,
                    help='minimum read Mapping quality score to consider')
parser.add_argument("--minconfirm", type=int, default=50,
                    help="minimum Pileup quality sum to confirm reference or SNP (with minfrac)")
parser.add_argument("--mingap", type=int, default=2000,
                    help="minimum contiguous uncovered (Gap) region to flag")
parser.add_argument("--minfrac", type=float, default=0.1,
                    help="minimum fraction of evidence to confirm presence (with minconfirm)")
parser.add_argument("--log", "-l",
                    help="log file")
parser.add_argument("reference",
                    help="reference FASTA file")
parser.add_argument("bam", nargs='+',
                    help="bam file of reads aligned to reference (sorted & indexed)")
args = parser.parse_args()

logfile = open(args.log, 'w') if args.log else sys.stdout




class Reference:
    def __init__(self, fasta):
        self.scaffolds = [scaffold for scaffold in kmertools.openSeqFile(fasta)]
        self.length = sum([len(s) for s in self.scaffolds])
        print >> logfile, "Reference has", len(self.scaffolds), "scaffolds containing", self.length, "bases"

class Sample:
    def __init__(self, reference, bamfile):
        self.reference = reference
        self.length = reference.length
        self.bamfile = bamfile

        # These are filled out after allele calling
        self.refmask = None
        self.coverage = None
        self.highCoverage = None
        self.strong = None
        self.weak = None
        self.gaps = []
        self.summaryStats = None

        self.process(bamfile)

    def process(self, bam):
        print >>logfile, "Processing", bam
        scaffolds = [Scaffold(scaffold.name, scaffold.seq.upper()) for scaffold in reference.scaffolds]
        with pysam.AlignmentFile(bamfile, "rb") as bam:
            for s in scaffolds:
                s.makePileups(bam)

        self.refmask = np.concatenate([s.refmask for s in scaffolds])
        self.coverage = np.concatenate([s.coverage for s in scaffolds])
        meanCoverage = self.coverage.sum() / float(self.length)
        medianCoverage = float(np.median(self.coverage))
        highCutoff = self.poissonCutoff(max(medianCoverage, 0.5))
        mingap = self.gapcutoff(meanCoverage)
        print >>logfile, "Total length=%d coverage=%.2f median=%.1f cutoff=%d mingap=%d" % (self.length, meanCoverage, medianCoverage, highCutoff, mingap),

        nonzero = self.coverage[self.coverage > 0]
        nonzeroCoverage = float(nonzero.sum()) / float(nonzero.size)
        print >>logfile, "nonzero=%d nzcoverage=%.2f" % (nonzero.size, nonzeroCoverage)
        logfile.flush()


        for s in scaffolds:
            s.callAlleles(highCutoff)
            self.gaps += s.findGaps(mingap)
            s.stats()

        self.highCoverage = np.concatenate([s.highCoverage for s in scaffolds])
        self.strong = np.concatenate([s.strong for s in scaffolds])
        self.weak = np.concatenate([s.weak for s in scaffolds])

        self.summaryStats = CallStats(self.length, self.refmask, self.coverage, self.strong, self.highCoverage, self.gaps)
        self.summaryStats.log("TOTAL")

        if args.table:
            with open(self.strainName() + ".tsv", 'w') as outfile:
                self.summaryStats.tabular(outfile, "TOTAL", True)
                for s in scaffolds:
                    s.summaryStats.tabular(outfile, s.name)
                self.summaryStats.tabular(outfile, "TOTAL")

    def poissonCutoff(self, mean, cutoff = 0.9999999):
        """Calculate the Poisson CDF and find where it reaches the cutoff"""
        i = 0
        ifact = 1.0
        expmean = math.exp(-mean)
        cdf = 0.0
        while cdf < cutoff:
            cdf += expmean * math.pow(mean, i) / ifact
            i += 1
            ifact *= i
        return i - 1

    def gapcutoff(self, coverage):
        return int(round(args.mingap / (1.0 - math.exp(-coverage))))

    def poisson0(self, mean, cutoff = 0.000001):
        gapcutoff
        math.log()
        return math.exp(-mean)


    def strainName(self):
        basename = os.path.basename(self.bamfile)
        if basename.lower().endswith(".bam"):
            basename = basename[:-4]
        return basename



class Scaffold:
    def __init__(self, name, sequence):
        self.name = name
        self.sequence = sequence
        self.length = len(sequence)
        self.pileups = np.array([Pileup(base) for base in sequence], dtype=np.dtype(object))
        # pileup stats
        self.refmask = np.zeros(self.length, dtype=np.uint8)
        self.coverage = np.zeros(self.length, dtype=np.uint32)
        # allele call stats
        self.strong = np.zeros(self.length, dtype=np.uint8)
        self.weak = np.zeros(self.length, dtype=np.uint8)
        # allele call arrays computed here
        self.highCoverage = None
        self.summaryStats = None
        self.gaps = []

    def makePileups(self, bam):
        print >>logfile, "Generating pileups for", self.name, "length=%d" % self.length,
        logfile.flush()

        for column in bam.pileup(self.name):
            refpos = column.reference_pos
            self.pileups[refpos].add_reads(column.pileups)

        for i in xrange(self.length):
            p = self.pileups[i]
            self.coverage[i] = p.depth()
            self.refmask[i] = p.refmask

        self.meancoverage = float(self.coverage.sum()) / float(self.length)

        print >>logfile, "coverage=%.2f" %self.meancoverage
        logfile.flush()

    def callAlleles(self, highCutoff):
        print >>logfile, "Calling alleles in", self.name,
        logfile.flush()

        self.highCoverage = self.coverage > highCutoff

        for i in xrange(self.length):
            if self.coverage[i] and not self.highCoverage[i]:
                p = self.pileups[i]
                p.callAlleles()
                self.strong[i] = p.strong
                self.weak[i] = p.weak


    def findGaps(self, mingap):
        lastCovered = -1
        for i in xrange(self.length):
            p = self.pileups[i]
            if p.strong or p.poormq():
                if lastCovered >= 0 and i - lastCovered > mingap:
                    self.gaps.append(Region(self.name, lastCovered+1, i-1))
                lastCovered = i
        if self.length - lastCovered >= mingap:
            self.gaps.append(Region(self.name, lastCovered + 1, self.length - 1))
        return self.gaps

    def stats(self):
        self.summaryStats = CallStats(self.length, self.refmask, self.coverage, self.strong, self.highCoverage, self.gaps)
        self.summaryStats.log()


    def process(self, bam):
        """Scan the pileups for each locus in the scaffold"""

        print >>logfile, "Processing", self.name, self
        last_covered = -1
        goodcoverage = 0

        for column in bam.pileup(self.name):
            refpos = column.reference_pos
            pileup = Pileup(self.sequence[refpos])
            pileup.add_reads(column.pileups)
            self.pileups[refpos] = pileup

            if args.verbose:
                refbase = self.sequence[refpos]
                print >> logfile, "Ref:", column.reference_name, refpos, refbase, column.nsegments
            goodcoverage += pileup.depth()
            if pileup.covered():
                covered += 1
                if pileup.strong():
                    confirmed += 1
                else:
                    # keep read info for covered, non-ref alleles
                    # self.insert_into_db(pileup)
                    if pileup.base_call():
                        snps += 1
                if refpos - last_covered > args.min_gap:
                    gap = (last_covered + 1, refpos - last_covered)
                    print >> logfile, "Coverage gap:", gap[0], gap[1]
                    gaps.append(gap)
                last_covered = refpos
            else:
                if pileup.unmappable():
                    unmapped += 1
                    # not a real gap, just can't map to this region
                    if refpos - last_covered > args.min_gap:
                        gap = (last_covered + 1, refpos - last_covered)
                        print >> logfile, "Coverage gap:", gap[0], gap[1]
                        gaps.append(gap)
                    last_covered = refpos
                    # if keep:
                    # keep all pileups anyway
                    # self.insert_into_db(pileup)

            # del pileup.reads

            if args.verbose:
                print >> logfile, pileup, pileup.strong

        coverage = float(goodcoverage) / float(length)
        mixed = covered - (confirmed + snps)

        print >> logfile, "good coverage: %.1fx" % (coverage,)
        print >> logfile, "covered: %d %.1f%%" % (covered, pct(covered, length))
        print >> logfile, "confirmed: %d %.2f%%" % (confirmed, pct(confirmed, covered))
        print >> logfile, "snps: %d %.3f%%" % (snps, pct(snps, covered))
        if snps > 0:
            snp_rate = "%.0f" % (float(covered) / float(snps))
            print >> logfile, "snp rate:", snp_rate
        else:
            snp_rate = ""
        print >> logfile, "mixed: %d %.3f%%" % (mixed, pct(mixed, covered))
        if mixed > 0:
            mixed_rate = float(covered) / float(mixed)
            if mixed_rate > 0:
                mixed_quality = math.log10(mixed_rate) * 10.0
            else:
                mixed_quality = 0
            mixed_rate = "%.0f" % (mixed_rate)
            print >> logfile, "mixed rate: %s Q%.0f" % (mixed_rate, mixed_quality)
        else:
            mixed_rate = ""
            mixed_quality = 0
        gap_total = sum([g[1] for g in gaps])
        print >> logfile, "gaps:", len(gaps), "totaling", gap_total
        print >> logfile, "unmapped: %d %.1f%%" % (unmapped, pct(unmapped, length))


class Pileup:
    """
    Class to process pileup information; that is,
    all the alignment information corresponding
    to a given reference coordinate.
    """

    # bases and their indicies (plus I for insertion, D for deletion)
    BASES = "ACGTID"
    A = 0
    C = 1
    G = 2
    T = 3
    I = 4
    D = 5
    ALLELES = len(BASES)
    ALLELE_MASKS = 1 << np.arange(ALLELES, dtype=np.int32)


    def __init__(self, refbase):
        """
        :param scaffold: reference sequence
        :param pos: coordinate in reference (0-based)
        :param pileup_reads: pileup objects from pysam
        """

        # first dimension is count, qualsum
        self.alleles = np.zeros((2, 6), dtype=np.int32)
        # index into allele array for reference base
        refindex = Pileup.BASES.find(refbase)
        self.refmask = (1 << refindex) if refindex >= 0 else 0
        self.bad = 0
        self.lowmq = 0
        self.strong = 0
        self.weak = 0

    def add_reads(self, pileup_reads):
        """Add a bunch of pysam.PileupRead objects to this pileup"""
        for read in pileup_reads:
            self.add_read(read)

    def add_read(self, read):
        """Add a pysam.PileupRead object to this pileup"""
        alignment = read.alignment

        # if this is a paired read, make sure the pairs are properly aligned
        if (not alignment.is_paired) or (not alignment.is_proper_pair):
            self.bad += 1
            return

        # restrict ourselves to full-length alignments (not clipped)
        if alignment.query_alignment_length != alignment.query_length:
            # alignment is clipped
            self.bad += 1
            return

        # check that inferred insert size is at least read length
        tlen = alignment.template_length
        if abs(tlen) < alignment.query_length:
            self.bad += 1
            return

        # get base quality (note this is next base if deletion)
        pos = read.query_position_or_next
        qual = alignment.query_qualities[pos]
        if qual < args.minqual:
            self.bad += 1
            return

        # check for decent mapping quality
        mq = alignment.mapping_quality
        if mq < args.minmq:
            self.lowmq += 1
            return

        if read.indel:
            if read.is_del:
                baseIndex = Pileup.D
                # if the base isn't there, base qual doesn't make sense
                qual = mq
            else:
                baseIndex = Pileup.I
        else:
            # base call must be real base (e.g., not N)
            base = alignment.query_sequence[pos]
            baseIndex = Pileup.BASES.find(base)
            if baseIndex < 0:
                self.bad += 1
                return

        # We're good! Update the pileup stats...
        q = min(qual, mq)
        self.alleles[0, baseIndex] += 1
        self.alleles[1, baseIndex] += q

    def depth(self):
        """Sum of all quality evidence"""
        return self.alleles[0].sum()

    def qual_total(self):
        """Sum of all quality evidence"""
        return self.alleles[1].sum()

    def total_depth(self):
        return self.depth() + self.bad

    def covered(self):
        """Does this pileup have enough data to consider this locus covered?"""
        return self.qual_total() >= args.minconfirm

    def poormq(self):
        """Good quality sequence, but can't be mapped"""
        return self.lowmq > (self.total_depth())

    def ref_qual(self):
        return self.alleles[1, self.refindex]

    def ref_fraction(self):
        """Fraction of evidence which supports reference base"""
        return float(self.ref_qual()) / float(self.qual_total())

    def confirmed(self):
        """Does this pileup confirm the reference?"""
        rq = self.ref_qual()
        return rq >= args.minconfirm and \
               (rq == self.qual_total() or self.ref_fraction() > args.consensus)

    def callAlleles(self):
        minconfirm = args.minconfirm
        minfrac = args.minfrac

        quals = self.alleles[1]
        qualsum = float(self.qual_total())
        qualfraction = quals / qualsum

        # Any alleles which have any evidence at all
        evidence = quals > 0
        self.weak = (evidence * Pileup.ALLELE_MASKS).sum()

        # Alleles with sufficient evidence to confirm
        confirmed = np.logical_and(quals > minconfirm, qualfraction > minfrac)
        self.strong = (confirmed * Pileup.ALLELE_MASKS).sum()

        if args.verbose and self.strong and self.refmask != self.strong:
            print self.refmask, self.alleles[0], self.alleles[1], qualfraction, self.weak, self.strong
        return



class CallStats:
    FIELDS = (("length", "%d"), ("coverage", "%.2f"), ("median", "%d"), ("covered", "%d"), ("coveredPct", "%.2f"), ("evenness", "%.2f"),
              ("confirmed", "%d"), ("confirmedPct", "%.2f"), ("snps", "%d"), ("snpPct", "%.2f"), ("multi", "%d"),
              ("multiPct", "%.2f"), ("high", "%d"), ("highPct", "%.2f"), ("gapCount", "%d"), ("gapLength", "%d"))
    #tab_line = "{ref}\t{bam}\t{chrom}\t{length:d}\t{goodcov:.2f}\t{covered:d}\t{pcovered:.1f}\t{confirmed:d}\t{pconfirmed:.2f}\t{snps:d}\t{psnps:.2f}\t{snprate}\t{mixed:d}\t{pmixed:.2f}\t{mixedrate}\t{mixedquality:.0f}\t{gaps:d}\t{gaptotal:d}\t{unmapped:d}\t{highcov:d}\t{highthresh:d}\n"

    def __init__(self, length, refmask, coverage, strong, high, gaps):
        confirmed = (strong & refmask)
        snps = (strong & ~refmask)
        multi = (strong & (strong - 1))
        self.length = length
        self.covered = np.count_nonzero(strong)
        self.coveredPct = pct(self.covered, length)
        self.coverage = float(coverage.sum()) / float(length)
        self.median = int(round(np.median(coverage)))
        lw = landerWaterman(self.coverage)
        self.evenness = self.coveredPct / (100.0 * lw * lw) if lw > 0 else 0
        self.confirmed = np.count_nonzero(confirmed)
        self.confirmedPct = pct(self.confirmed, self.covered)
        self.snps = np.count_nonzero(snps)
        self.snpPct = pct(self.snps, self.covered)
        self.multi = np.count_nonzero(multi)
        self.multiPct = pct(self.multi, self.snps)
        self.high = np.count_nonzero(high)
        self.highPct = pct(self.high, length)
        self.gapCount = len(gaps)
        self.gapLength = sum([g.length for g in gaps])

        self.values = (self.length, self.coverage, self.median, self.covered, self.coveredPct, self.evenness, self.confirmed,
                       self.confirmedPct, self.snps, self.snpPct, self.multi, self.multiPct, self.high, self.highPct,
                       self.gapCount, self.gapLength)

    def log(self, name=None):
        if name:
            first = [name]
        else:
            first = []
        s = ' '.join(first + [(field[0] + '=' + field[1]) % value for field, value in zip(CallStats.FIELDS, self.values)])
        print >>logfile, s
        logfile.flush()

    def tabular(self, out, name, header=False):
        if header:
            print >>out, "\t".join(["name"] + [field for field, fmt in CallStats.FIELDS])
        else:
            print >>out, "\t".join([name] + [ff[1] % value for ff, value in zip(CallStats.FIELDS, self.values)])



class CompareStats:
    FIELDS = [("common", "%d"), ("commonPct", "%.2f"), ("disagree", "%d"), ("disagreePct", "%.2f"), ("variants", "%d"), \
              ("variantPct", "%.2f"), ("commonVariant", "%d"), ("commonVariantPct", "%.2f"), ("variantAgree", "%d"),
              ("variantAgreePct", "%.2f"), ("AnotB", "%d"), ("AnotBpct", "%.2f"), \
              ("AnotBweak", "%d"), ("AnotBweakPct", "%.2f"), ("BnotA", "%d"), ("BnotApct", "%.2f"), \
              ("BnotAweak", "%d"), ("BnotAweakPct", "%.2f"), ("Agaps", "%d"), ("AsharedGaps", "%d"), ("AgapPct", "%.2f"), \
              ("Bgaps", "%d"), ("BsharedGaps", "%d"), ("BgapPct", "%.2f")]
    #tab_line = "{ref}\t{bam}\t{chrom}\t{length:d}\t{goodcov:.2f}\t{covered:d}\t{pcovered:.1f}\t{confirmed:d}\t{pconfirmed:.2f}\t{snps:d}\t{psnps:.2f}\t{snprate}\t{mixed:d}\t{pmixed:.2f}\t{mixedrate}\t{mixedquality:.0f}\t{gaps:d}\t{gaptotal:d}\t{unmapped:d}\t{highcov:d}\t{highthresh:d}\n"

    def __init__(self, strainA, strainB):
        self.strainA = strainA
        self.strainB = strainB
        self.values = self.compare(strainA, strainB)
        self.values += self.compareGaps()

    def compare(self, strainA, strainB):
        print >>logfile, "Comparing", strainA.bamfile, "versus",
        logfile.flush()

        # locations where both have a call in common
        common, commonCount, commonPct = self.compareThing(np.ones_like(strainA.refmask), np.logical_and(strainA.strong, strainB.strong), "common")

        # common locations where A & B disagree
        disagree, disagreeCount, disagreePct = self.compareThing(common, strainA.strong != strainB.strong, "disagree")

        # common locations where either has a variant from reference
        variants, variantCount, variantPct = self.compareThing(common, ((strainA.strong | strainB.strong) & ~strainA.refmask) > 0, "variant")

        # variant locations where both have a shared variant
        commonVariants, commonVariantCount, commonVariantPct = self.compareThing(variants, (strainA.strong & strainB.strong) > 0, "commonvariant")

        # variant locations where both agree
        variantAgree, variantAgreeCount, variantAgreePct = self.compareThing(variants, strainA.strong == strainB.strong, "variantAgree")

        # variant in A but not B
        AnotB, AnotBcount, AnotBpct = self.compareThing(variants, (strainA.strong & ~strainB.strong & ~strainA.refmask) > 0, "AnotB")

        # variant in A but not B weakly
        AnotBweak, AnotBweakCount, AnotBweakPct = self.compareThing(variants, (strainA.strong & ~strainB.weak & ~strainA.refmask) > 0, "AnotBweak")

        # variant in B not A
        BnotA, BnotAcount, BnotApct = self.compareThing(variants, (strainB.strong & ~strainA.strong & ~strainA.refmask) > 0, "BnotA")

        # variant in B not A weakly
        BnotAweak, BnotAweakCount, BnotAweakPct = self.compareThing(variants, (strainB.strong & ~strainA.weak & ~strainA.refmask) > 0, "BnotAweak")

        return [commonCount, commonPct, disagreeCount, disagreePct, variantCount, variantPct,
                commonVariantCount, commonVariantPct,
                variantAgreeCount, variantAgreePct, AnotBcount, AnotBpct, AnotBweakCount, AnotBweakPct,
                BnotAcount, BnotApct, BnotAweakCount, BnotAweakPct]

    def compareThing(self, common, thing, label):
        commonCount = np.count_nonzero(common)
        commonThings = np.logical_and(common, thing)
        commonThingsCount = np.count_nonzero(commonThings)
        percent = pct(commonThingsCount, commonCount)
        return commonThings, commonThingsCount, percent

    def compareGaps(self):
        Agaps = self.strainA.gaps
        Bgaps = self.strainB.gaps
        Alength = sum([g.length for g in Agaps])
        Blength = sum([g.length for g in Bgaps])
        Ashared = filter(lambda ag: any([ag.overlaps(bg) for bg in Bgaps]), Agaps)
        AsharedLength = sum([g.length for g in Ashared])
        Bshared = filter(lambda bg: any([bg.overlaps(ag) for ag in Agaps]), Bgaps)
        BsharedLength = sum([g.length for g in Bshared])
        return [Alength, AsharedLength, pct(AsharedLength, Alength), Blength, BsharedLength, pct(BsharedLength, Blength)]

    def log(self, name):
        s = ' '.join([name] + [(field[0] + '=' + field[1]) % value for field, value in zip(CompareStats.FIELDS, self.values)])
        print >>logfile, s
        logfile.flush()

    def tabular(self, out, name, header=False):
        if header:
            print >>out, "\t".join(["name"] + [field for field, fmt in CompareStats.FIELDS])
        else:
            print >>out, "\t".join([name] + [ff[1] % value for ff, value in zip(CompareStats.FIELDS, self.values)])


class Region:
    def __init__(self, id, start, stop):
        self.id = id
        self.start = start
        self.stop = stop
        self.length = stop - start + 1
        assert self.length > 0, "Region length must be positive"

    def overlaps(self, other):
        return self.id == other.id and other.start <= self.stop and other.stop >= self.start

    def distance(self, other):
        assert self.id == other.id
        return min(abs(self.end - other.start), abs(other.end - self.start)) if not self.overlap() else 0

    def combine(self, other):
        assert self.id == other.id
        return Region(self.id, min(self.start, other.start), max(self.end, other.end))

    def __repr__(self):
        return "<%s %d-%d>" % (self.id, self.start, self.stop)

def landerWaterman(coverage):
    """
    Use Lander-Waterman relation to compute expect fraction of genome covered given mean coverage.
    :param coverage: mean coverage
    :return: expected fraction of genome covered
    """
    return 1.0 - math.exp(-coverage)

def pct(numerator, denominator, precision=None):
    """Makes into a percent, avoiding division by zero"""
    if numerator > 0 and denominator > 0:
        value = (100.0 * numerator) / denominator
    else:
        value = 0.0
    if precision is not None:
        value = round(value, precision)
    return value

##################################
### Main
##################################

print >>logfile, "Loading reference genome"
logfile.flush()
print args.reference
reference = Reference(args.reference)
baseline = None
compareFile = None

for bamfile in args.bam:
    sample = Sample(reference, bamfile)
    sampleName = sample.strainName()
    if args.compare:
        if not baseline:
            baseline = sample
        else:
            compare = CompareStats(baseline, sample)
            compare.log(sampleName)
            if not compareFile:
                compareFile = open(baseline.strainName() + ".compare.tsv", 'w')
                compare.tabular(compareFile, "sample", True)
            compare.tabular(compareFile, sampleName)
