#!/usr/bin/env python
"""StrainGR"""

import argparse
import math
import os
import sys
from datetime import date
import numpy as np
import pysam
import kmertools



parser = argparse.ArgumentParser(description="""
Strain Genome Recovery (StrainGR) Tool
""")
parser.add_argument("--table", "-t", action="store_true",
                    help="generate tabular output in <bambasename>.tsv")
parser.add_argument("--compare", "-c", action='store_true',
                    help="compare against first listed strain bam")
parser.add_argument("--delta", "-d", action='store_true',
                    help="log specific differences between strains")
parser.add_argument("--verbose", "-v",
                    help="increase output verbosity", action="store_true")
parser.add_argument("--minqual", type=int, default=5,
                    help='minimum base Quality score to consider')
parser.add_argument("--minmq", type=int, default=5,
                    help='minimum read Mapping quality score to consider')
parser.add_argument("--minconfirm", type=int, default=50,
                    help="minimum Pileup quality sum to confirm reference or SNP (with minfrac)")
parser.add_argument("--mingap", type=int, default=2000,
                    help="minimum contiguous uncovered (Gap) region to flag")
parser.add_argument("--minfrac", type=float, default=0.1,
                    help="minimum fraction of evidence to confirm presence (with minconfirm)")
parser.add_argument("--log", "-l",
                    help="log file")
parser.add_argument("reference",
                    help="reference FASTA file")
parser.add_argument("bam", nargs='+',
                    help="bam file of reads aligned to reference (sorted & indexed)")
args = parser.parse_args()

logfile = open(args.log, 'w') if args.log else sys.stdout




class Reference:
    """
    Holds reference sequence information...scaffolds in Biopython SeqRecord objects.
    """
    def __init__(self, fasta):
        self.scaffolds = [scaffold for scaffold in kmertools.openSeqFile(fasta)]
        self.length = sum([len(s) for s in self.scaffolds])
        print >> logfile, "Reference has", len(self.scaffolds), "scaffolds containing", self.length, "bases"

class Sample:
    """
    Collects and holds information about alignments of sample reads (from a supplied BAM file) against a reference.
    """
    def __init__(self, reference, bamfile):
        self.reference = reference
        self.length = reference.length
        self.bamfile = bamfile

        # These are filled out after allele calling. Most are numpy arrays with one element for each genomic position.
        self.refmask = None
        self.coverage = None
        self.highCoverage = None
        self.strong = None
        self.weak = None
        self.gaps = []
        self.summaryStats = None

        self.process(bamfile)

    def process(self, bam):
        """
        Does the heavy lifting of scanning the BAM file. The basic flow is:
        1) process the alignments on a scaffold-by-scaffold basis, generating ScaffoldsPileups objects
        2) generate some summary statistics and heuristic thresholds (e.g., read depth cutoff)
        3) scan through the ScaffoldPileups and make calls
        4) compute summary call statistics for each scaffold as well as the total for the genome

        :param bam: BAM file containing reads for a sample aligned to the reference genome.
        """
        print >>logfile, "Processing", bam

        # Create ScaffoldPileups objects for each scaffold, and scan the bam to fill them out.
        scaffolds = [ScaffoldPileups(scaffold.name, scaffold.seq.upper()) for scaffold in reference.scaffolds]
        with pysam.AlignmentFile(bamfile, "rb") as bam:
            for s in scaffolds:
                s.makePileups(bam)

        # Pull together essential stats for the whole genome so we can set thresholds
        self.refmask = np.concatenate([s.refmask for s in scaffolds])
        self.coverage = np.concatenate([s.coverage for s in scaffolds])
        meanCoverage = float(self.coverage.sum()) / float(self.length)
        medianCoverage = float(np.median(self.coverage))

        # Based on coverage, set a high limit for plausible coverage of a given position based on Poisson probability;
        # because we are dealing with metagenomic samples, too improbabily high coverage is likely a result of reads
        # from other organisms aligning to conserved regions, so we can't make a confident call on our target organism.
        # Here, we use the median coverage rather than the mean since our coverage might be dominated by conserved regions.
        highCutoff = self.poissonCutoff(max(medianCoverage, 0.5))

        # When we look for blocks without coverage (possible large deletions from recombination events), we attempt
        # to scale the minimum size to be significant, because at very low coverage, we expect large uncovered regions.
        mingap = self.gapCutoff(meanCoverage)
        print >>logfile, "Total length=%d coverage=%.2f median=%.1f cutoff=%d mingap=%d" % (self.length, meanCoverage, medianCoverage, highCutoff, mingap),

        nonzero = self.coverage[self.coverage > 0]
        nonzeroCoverage = float(nonzero.sum()) / float(nonzero.size)
        print >>logfile, "nonzero=%d nzcoverage=%.2f" % (nonzero.size, nonzeroCoverage)
        logfile.flush()

        # Scan through ScaffoldPileups, make calls using the thresholds set above, and compute summary stats
        for s in scaffolds:
            s.callAlleles(highCutoff)
            self.gaps += s.findGaps(mingap)
            s.stats()

        # Now gather the essentials for the whole genome so we can compute summary stats.
        self.highCoverage = np.concatenate([s.highCoverage for s in scaffolds])
        self.strong = np.concatenate([s.strong for s in scaffolds])
        self.weak = np.concatenate([s.weak for s in scaffolds])

        self.summaryStats = CallStats(self.length, self.refmask, self.coverage, self.strong, self.highCoverage, self.gaps)
        self.summaryStats.log("TOTAL")

        # If tabular output has been requested, create a tab-separated values output file with entries
        # for each scaffold and total.
        if args.table:
            with open(self.strainName() + ".tsv", 'w') as outfile:
                self.summaryStats.tabular(outfile, "TOTAL", True)
                for s in scaffolds:
                    s.summaryStats.tabular(outfile, s.name)
                self.summaryStats.tabular(outfile, "TOTAL")

    def poissonCutoff(self, mean, cutoff = 0.9999999):
        """
        Calculate the Poisson CDF and find where it reaches the cutoff. Default cutoff is one part in 10M, so
        not likely to occur in a typical bacterial genome.
        """
        i = 0
        ifact = 1.0
        expmean = math.exp(-mean)
        cdf = 0.0
        while cdf < cutoff:
            cdf += expmean * math.pow(mean, i) / ifact
            i += 1
            ifact *= i
        return i - 1

    def gapCutoff(self, coverage):
        """
        Attempt to scale the minimum significant uncoveraged region by coverage (for low coverage)
        :param coverage: expected coverage
        :return: scaled minimum gap (using args.mingap as baseline)
        """
        return int(round(args.mingap / landerWaterman(coverage)))

    def strainName(self):
        """
        Derive "strain name" from basename of bam file...used to derive other output file names
        :return: basename suitable for appending output file extensions
        """
        basename = os.path.basename(self.bamfile)
        if basename.lower().endswith(".bam"):
            basename = basename[:-4]
        return basename



class ScaffoldPileups:
    """
    Object to compute an hold all the pileups for each location in an input genome element (scaffold).
    Detail information is contained in Pileup objects for each locus; summary information is in numpy arrays (coverage,
    strong, weak, ...) with one element per locus.
    """
    def __init__(self, name, sequence):
        self.name = name
        self.sequence = sequence
        self.length = len(sequence)

        # large memory requirement done this way (object per locus). could be refactored by having multidimensional
        # array with allele evidence for the whole scaffold.
        self.pileups = np.array([Pileup(base) for base in sequence], dtype=np.dtype(object))
        # pileup stats
        self.refmask = np.zeros(self.length, dtype=np.uint8)
        self.coverage = np.zeros(self.length, dtype=np.uint32)
        # allele calls: strong and weak are both byte arrays with one bit set for each present allele (A,C,G,T + ins + del)
        self.strong = np.zeros(self.length, dtype=np.uint8)
        self.weak = np.zeros(self.length, dtype=np.uint8)
        # allele call arrays computed here
        self.highCoverage = None
        self.summaryStats = None
        # list of Region objects for uncovered regions
        self.gaps = []

    def makePileups(self, bam):
        """
        Loop over loci and add the info from each covering read. Also sets count of "good" coverage and refmask, which
        is the bitmask corresponding to the reference allele for each comparison later.

        :param bam: open bam file (pysam object).
        """
        print >>logfile, "Generating pileups for", self.name, "length=%d" % self.length,
        logfile.flush()

        for column in bam.pileup(self.name):
            refpos = column.reference_pos
            self.pileups[refpos].add_reads(column.pileups)

        for i in xrange(self.length):
            p = self.pileups[i]
            self.coverage[i] = p.depth()
            self.refmask[i] = p.refmask

        self.meancoverage = float(self.coverage.sum()) / float(self.length)

        print >>logfile, "coverage=%.2f" %self.meancoverage
        logfile.flush()

    def callAlleles(self, highCutoff):
        """
        Loop over loci compiling evidence for alleles present. Ignore pileups deeper than highCutoff, because those
        probably contain alignments from other organisms (e.g., this might be a conserved region).

        "strong" means there is suffiently strong evidnece to "call" the allele as present
        "weak" means any evidence at all supporting the allele, even if not strong enought to be strong

        :param highCutoff: max pileup depth
        """
        print >>logfile, "Calling alleles in", self.name,
        logfile.flush()

        self.highCoverage = self.coverage > highCutoff

        for i in xrange(self.length):
            if self.coverage[i] and not self.highCoverage[i]:
                p = self.pileups[i]
                p.callAlleles()
                self.strong[i] = p.strong
                self.weak[i] = p.weak


    def findGaps(self, mingap):
        """
        Find coverage gaps, indicating possible deleted portions in our strain vs the reference (e.g., possible
        recombination events). Gaps are defined here as insufficient evidence to call anything, taking into account
        regions of poor mapping quality won't be called as a matter of course (but may still be "covered").

        :param mingap: don't report gaps unless they consist of this many consecutive loci which can't be called
        :return: list of Region objects corresponding to the coverage gaps, also set as self.gaps
        """
        lastCovered = -1
        for i in xrange(self.length):
            p = self.pileups[i]
            # we break at either a call or a location which has sufficient coverage at poor mapping quality (repeats)
            if p.strong or p.poormq():
                if lastCovered >= 0 and i - lastCovered > mingap:
                    self.gaps.append(Region(self.name, lastCovered+1, i-1))
                lastCovered = i
        if self.length - lastCovered >= mingap:
            self.gaps.append(Region(self.name, lastCovered + 1, self.length - 1))
        return self.gaps

    def stats(self):
        """
        Populate and report call statistics for this scaffold.
        """
        self.summaryStats = CallStats(self.length, self.refmask, self.coverage, self.strong, self.highCoverage, self.gaps)
        self.summaryStats.log()


class Pileup:
    """
    Class to process pileup information; that is,
    all the alignment information corresponding
    to a given reference coordinate.
    """

    # bases and their indicies (plus I for insertion, D for deletion)
    BASES = "ACGTID"
    A = 0
    C = 1
    G = 2
    T = 3
    I = 4
    D = 5
    ALLELES = len(BASES)
    ALLELE_MASKS = 1 << np.arange(ALLELES, dtype=np.int32)


    def __init__(self, refbase):
        """
        :param scaffold: reference sequence
        :param pos: coordinate in reference (0-based)
        :param pileup_reads: pileup objects from pysam
        """

        # first dimension is count, qualsum
        self.alleles = np.zeros((2, 6), dtype=np.int32)
        # index into allele array for reference base
        refindex = Pileup.BASES.find(refbase)
        self.refmask = (1 << refindex) if refindex >= 0 else 0
        self.bad = 0
        self.lowmq = 0
        self.strong = 0
        self.weak = 0

    def add_reads(self, pileup_reads):
        """Add a bunch of pysam.PileupRead objects to this pileup"""
        for read in pileup_reads:
            self.add_read(read)

    def add_read(self, read):
        """Add a pysam.PileupRead object to this pileup"""
        alignment = read.alignment

        # if this is a paired read, make sure the pairs are properly aligned
        if (not alignment.is_paired) or (not alignment.is_proper_pair):
            self.bad += 1
            return

        # restrict ourselves to full-length alignments (not clipped)
        if alignment.query_alignment_length != alignment.query_length:
            # alignment is clipped
            self.bad += 1
            return

        # check that inferred insert size is at least read length
        tlen = alignment.template_length
        if abs(tlen) < alignment.query_length:
            self.bad += 1
            return

        # get base quality (note this is next base if deletion)
        pos = read.query_position_or_next
        qual = alignment.query_qualities[pos]
        if qual < args.minqual:
            self.bad += 1
            return

        # check for decent mapping quality
        mq = alignment.mapping_quality
        if mq < args.minmq:
            self.lowmq += 1
            return

        if read.indel:
            if read.is_del:
                baseIndex = Pileup.D
                # if the base isn't there, base qual doesn't make sense
                qual = mq
            else:
                baseIndex = Pileup.I
        else:
            # base call must be real base (e.g., not N)
            base = alignment.query_sequence[pos]
            baseIndex = Pileup.BASES.find(base)
            if baseIndex < 0:
                self.bad += 1
                return

        # We're good! Update the pileup stats...
        q = min(qual, mq)
        self.alleles[0, baseIndex] += 1
        self.alleles[1, baseIndex] += q

    def depth(self):
        """Sum of all quality evidence"""
        return self.alleles[0].sum()

    def qual_total(self):
        """Sum of all quality evidence"""
        return self.alleles[1].sum()

    def total_depth(self):
        return self.depth() + self.bad

    def covered(self):
        """Does this pileup have enough data to consider this locus covered?"""
        return self.qual_total() >= args.minconfirm

    def poormq(self):
        """Good quality sequence, but can't be mapped"""
        return self.lowmq > (self.total_depth())

    def ref_qual(self):
        return self.alleles[1, self.refindex]

    def ref_fraction(self):
        """Fraction of evidence which supports reference base"""
        return float(self.ref_qual()) / float(self.qual_total())

    def confirmed(self):
        """Does this pileup confirm the reference?"""
        rq = self.ref_qual()
        return rq >= args.minconfirm and \
               (rq == self.qual_total() or self.ref_fraction() > args.consensus)

    def callAlleles(self):
        minconfirm = args.minconfirm
        minfrac = args.minfrac

        quals = self.alleles[1]
        qualsum = float(self.qual_total())
        qualfraction = quals / qualsum

        # Any alleles which have any evidence at all
        evidence = quals > 0
        self.weak = (evidence * Pileup.ALLELE_MASKS).sum()

        # Alleles with sufficient evidence to confirm
        confirmed = np.logical_and(quals > minconfirm, qualfraction > minfrac)
        self.strong = (confirmed * Pileup.ALLELE_MASKS).sum()

        if args.verbose and self.strong and self.refmask != self.strong:
            print self.refmask, self.alleles[0], self.alleles[1], qualfraction, self.weak, self.strong
        return



class CallStats:
    FIELDS = (("length", "%d"), ("coverage", "%.2f"), ("median", "%d"), ("covered", "%d"), ("coveredPct", "%.2f"), ("evenness", "%.2f"),
              ("confirmed", "%d"), ("confirmedPct", "%.2f"), ("snps", "%d"), ("snpPct", "%.2f"), ("multi", "%d"),
              ("multiPct", "%.2f"), ("high", "%d"), ("highPct", "%.2f"), ("gapCount", "%d"), ("gapLength", "%d"))
    #tab_line = "{ref}\t{bam}\t{chrom}\t{length:d}\t{goodcov:.2f}\t{covered:d}\t{pcovered:.1f}\t{confirmed:d}\t{pconfirmed:.2f}\t{snps:d}\t{psnps:.2f}\t{snprate}\t{mixed:d}\t{pmixed:.2f}\t{mixedrate}\t{mixedquality:.0f}\t{gaps:d}\t{gaptotal:d}\t{unmapped:d}\t{highcov:d}\t{highthresh:d}\n"

    def __init__(self, length, refmask, coverage, strong, high, gaps):
        confirmed = (strong & refmask)
        snps = (strong & ~refmask)
        multi = (strong & (strong - 1))
        self.length = length
        self.covered = np.count_nonzero(strong)
        self.coveredPct = pct(self.covered, length)
        self.coverage = float(coverage.sum()) / float(length)
        self.median = int(round(np.median(coverage)))
        lw = landerWaterman(self.coverage)
        self.evenness = self.coveredPct / (100.0 * lw * lw) if lw > 0 else 0
        self.confirmed = np.count_nonzero(confirmed)
        self.confirmedPct = pct(self.confirmed, self.covered)
        self.snps = np.count_nonzero(snps)
        self.snpPct = pct(self.snps, self.covered)
        self.multi = np.count_nonzero(multi)
        self.multiPct = pct(self.multi, self.snps)
        self.high = np.count_nonzero(high)
        self.highPct = pct(self.high, length)
        self.gapCount = len(gaps)
        self.gapLength = sum([g.length for g in gaps])

        self.values = (self.length, self.coverage, self.median, self.covered, self.coveredPct, self.evenness, self.confirmed,
                       self.confirmedPct, self.snps, self.snpPct, self.multi, self.multiPct, self.high, self.highPct,
                       self.gapCount, self.gapLength)

    def log(self, name=None):
        if name:
            first = [name]
        else:
            first = []
        s = ' '.join(first + [(field[0] + '=' + field[1]) % value for field, value in zip(CallStats.FIELDS, self.values)])
        print >>logfile, s
        logfile.flush()

    def tabular(self, out, name, header=False):
        if header:
            print >>out, "\t".join(["name"] + [field for field, fmt in CallStats.FIELDS])
        else:
            print >>out, "\t".join([name] + [ff[1] % value for ff, value in zip(CallStats.FIELDS, self.values)])



class CompareStats:
    FIELDS = [("common", "%d"), ("commonPct", "%.2f"), ("disagree", "%d"), ("disagreePct", "%.2f"), ("variants", "%d"), \
              ("variantPct", "%.2f"), ("commonVariant", "%d"), ("commonVariantPct", "%.2f"), ("variantAgree", "%d"),
              ("variantAgreePct", "%.2f"), ("AnotB", "%d"), ("AnotBpct", "%.2f"), \
              ("AnotBweak", "%d"), ("AnotBweakPct", "%.2f"), ("BnotA", "%d"), ("BnotApct", "%.2f"), \
              ("BnotAweak", "%d"), ("BnotAweakPct", "%.2f"), ("Agaps", "%d"), ("AsharedGaps", "%d"), ("AgapPct", "%.2f"), \
              ("Bgaps", "%d"), ("BsharedGaps", "%d"), ("BgapPct", "%.2f")]
    #tab_line = "{ref}\t{bam}\t{chrom}\t{length:d}\t{goodcov:.2f}\t{covered:d}\t{pcovered:.1f}\t{confirmed:d}\t{pconfirmed:.2f}\t{snps:d}\t{psnps:.2f}\t{snprate}\t{mixed:d}\t{pmixed:.2f}\t{mixedrate}\t{mixedquality:.0f}\t{gaps:d}\t{gaptotal:d}\t{unmapped:d}\t{highcov:d}\t{highthresh:d}\n"

    def __init__(self, strainA, strainB):
        self.strainA = strainA
        self.strainB = strainB
        self.values = self.compare(strainA, strainB)
        self.values += self.compareGaps()

    def compare(self, strainA, strainB):
        print >>logfile, "Comparing", strainA.bamfile, "versus",
        logfile.flush()

        # locations where both have a call in common
        common, commonCount, commonPct = self.compareThing(np.ones_like(strainA.refmask), np.logical_and(strainA.strong, strainB.strong), "common")

        # common locations where A & B disagree
        disagree, disagreeCount, disagreePct = self.compareThing(common, strainA.strong != strainB.strong, "disagree")

        # common locations where either has a variant from reference
        variants, variantCount, variantPct = self.compareThing(common, ((strainA.strong | strainB.strong) & ~strainA.refmask) > 0, "variant")

        # variant locations where both have a shared variant
        commonVariants, commonVariantCount, commonVariantPct = self.compareThing(variants, (strainA.strong & strainB.strong) > 0, "commonvariant")

        # variant locations where both agree
        variantAgree, variantAgreeCount, variantAgreePct = self.compareThing(variants, strainA.strong == strainB.strong, "variantAgree")

        # variant in A but not B
        AnotB, AnotBcount, AnotBpct = self.compareThing(variants, (strainA.strong & ~strainB.strong & ~strainA.refmask) > 0, "AnotB")

        # variant in A but not B weakly
        AnotBweak, AnotBweakCount, AnotBweakPct = self.compareThing(variants, (strainA.strong & ~strainB.weak & ~strainA.refmask) > 0, "AnotBweak")

        # variant in B not A
        BnotA, BnotAcount, BnotApct = self.compareThing(variants, (strainB.strong & ~strainA.strong & ~strainA.refmask) > 0, "BnotA")

        # variant in B not A weakly
        BnotAweak, BnotAweakCount, BnotAweakPct = self.compareThing(variants, (strainB.strong & ~strainA.weak & ~strainA.refmask) > 0, "BnotAweak")

        return [commonCount, commonPct, disagreeCount, disagreePct, variantCount, variantPct,
                commonVariantCount, commonVariantPct,
                variantAgreeCount, variantAgreePct, AnotBcount, AnotBpct, AnotBweakCount, AnotBweakPct,
                BnotAcount, BnotApct, BnotAweakCount, BnotAweakPct]

    def compareThing(self, common, thing, label):
        commonCount = np.count_nonzero(common)
        commonThings = np.logical_and(common, thing)
        commonThingsCount = np.count_nonzero(commonThings)
        percent = pct(commonThingsCount, commonCount)
        return commonThings, commonThingsCount, percent

    def compareGaps(self):
        Agaps = self.strainA.gaps
        Bgaps = self.strainB.gaps
        Alength = sum([g.length for g in Agaps])
        Blength = sum([g.length for g in Bgaps])
        Ashared = filter(lambda ag: any([ag.overlaps(bg) for bg in Bgaps]), Agaps)
        AsharedLength = sum([g.length for g in Ashared])
        Bshared = filter(lambda bg: any([bg.overlaps(ag) for ag in Agaps]), Bgaps)
        BsharedLength = sum([g.length for g in Bshared])
        return [Alength, AsharedLength, pct(AsharedLength, Alength), Blength, BsharedLength, pct(BsharedLength, Blength)]

    def log(self, name):
        s = ' '.join([name] + [(field[0] + '=' + field[1]) % value for field, value in zip(CompareStats.FIELDS, self.values)])
        print >>logfile, s
        logfile.flush()

    def tabular(self, out, name, header=False):
        if header:
            print >>out, "\t".join(["name"] + [field for field, fmt in CompareStats.FIELDS])
        else:
            print >>out, "\t".join([name] + [ff[1] % value for ff, value in zip(CompareStats.FIELDS, self.values)])


class Region:
    def __init__(self, id, start, stop):
        self.id = id
        self.start = start
        self.stop = stop
        self.length = stop - start + 1
        assert self.length > 0, "Region length must be positive"

    def overlaps(self, other):
        return self.id == other.id and other.start <= self.stop and other.stop >= self.start

    def distance(self, other):
        assert self.id == other.id
        return min(abs(self.end - other.start), abs(other.end - self.start)) if not self.overlap() else 0

    def combine(self, other):
        assert self.id == other.id
        return Region(self.id, min(self.start, other.start), max(self.end, other.end))

    def __repr__(self):
        return "<%s %d-%d>" % (self.id, self.start, self.stop)

def landerWaterman(coverage):
    """
    Use Lander-Waterman relation to compute expect fraction of genome covered given mean coverage.
    :param coverage: mean coverage
    :return: expected fraction of genome covered
    """
    return 1.0 - math.exp(-coverage)

def pct(numerator, denominator, precision=None):
    """Makes into a percent, avoiding division by zero"""
    if numerator > 0 and denominator > 0:
        value = (100.0 * numerator) / denominator
    else:
        value = 0.0
    if precision is not None:
        value = round(value, precision)
    return value

##################################
### Main
##################################

print >>logfile, "Loading reference genome"
logfile.flush()
print args.reference
reference = Reference(args.reference)
baseline = None
compareFile = None

for bamfile in args.bam:
    sample = Sample(reference, bamfile)
    sampleName = sample.strainName()
    if args.compare:
        if not baseline:
            baseline = sample
        else:
            compare = CompareStats(baseline, sample)
            compare.log(sampleName)
            if not compareFile:
                compareFile = open(baseline.strainName() + ".compare.tsv", 'w')
                compare.tabular(compareFile, "sample", True)
            compare.tabular(compareFile, sampleName)
